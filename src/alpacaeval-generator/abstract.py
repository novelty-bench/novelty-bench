# loads a llama-2-hf model - has it underspecify alpacaeval instances using vllm 
# generates inferences for the underspecified prompts using gemma. if says 'im sorry' reject the sample. 

from vllm import LLM, SamplingParams 
from datasets import load_dataset
import torch
import os
import tqdm
import json
import argparse
        
ABSTRACT_INSTRUCTION = '''You are given an instruction: Modify it such that it can seek more diverse responses. 
                        The question should be short (not more than 1 line) and should not explicitly ask the user to respond with "diverse/alternate/creative or detailed responses".
                        Your question should not ask the user to respond with details. Only return a single modified question. Do not return anything else (no explainations or candidate answers). 
                        Try to maintain the format of the question as much as possible. '''

SYSTEM_INSTRUCTION = '''Respond to the given instruction using any additional input context (if any).'''

def model_wise_formatter(model_type, prompts, abstract):
    BASE_INSTRUCTION = ABSTRACT_INSTRUCTION if abstract else SYSTEM_INSTRUCTION
    if 'llama2' in model_type:
        return [f"""<s> [INST] <<SYS>> {BASE_INSTRUCTION} <</SYS>>{prompt} [/INST]""" for prompt in prompts]
    elif 'gemma2' in model_type:
        return [f"""<bos><start_of_turn>user {BASE_INSTRUCTION}<end_of_turn><start_of_turn>model {prompt}""" for prompt in prompts]


def dump_predictions(prompts, model_responses, write_path, generator, dataset, abstract):
    datasplit = 'abstract' if abstract else 'vanilla'
    dump = []
    for prompt, model_response in zip(prompts, model_responses):
        dump.append({"instruction": prompt, 
                     "output": model_response, 
                     "generator": generator, 
                     "dataset": dataset, 
                     "datasplit": datasplit})
    
    with open(write_path + '.json', 'w') as file:
        json.dump(dump, file, indent=4) 

def read_file(prompt_path):
    with open(prompt_path, 'r') as file:
        records = json.load(file)
        outputs = [record['output'] for record in records] # abstracted outputs generated by some generator
        return outputs 
        

def inference(test_inputs, model, sampling_params):
    with tqdm.tqdm(torch.no_grad()):
        intermediate_outputs =  model.generate(test_inputs, sampling_params)
        predictions = [intermediate_output.outputs[0].text for intermediate_output in intermediate_outputs]
    return predictions 

def load_model(args):    
    model = LLM(args.model_path)
    print('Loaded model with VLLM ...')
    sampling_params = SamplingParams(max_tokens = 1024, logprobs=None)
    return model, sampling_params


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, default=None, help='the model being used to abstract the prompts.')
    parser.add_argument("--prompt_path", type=str, default=None, help='prompts you want to infer on')
    parser.add_argument("--model_type", type=str, help='Required to format the prompts in accordance with the model class.')
    parser.add_argument("--output_path", type=str, default = './abstracted_prompts')
    parser.add_argument("--abstract", action='store_true', default=False, help='flag to indicate if the script is being used for underspecifying prompts.')
    parser.add_argument("--max_samples", type = int, default = 5000)
    parser.add_argument("--dataset", type=str, default='alpacaeval')
    
    args = parser.parse_args()
    model, sampling_params = load_model(args)

    if args.abstract:
        dataset = load_dataset("tatsu-lab/alpaca")["train"]
        inputs = [f'{instruction}  {input}' for instruction, input in zip(dataset['instruction'], dataset['input'])][:args.max_samples]      
        test_inputs = model_wise_formatter(args.model_type, inputs, args.abstract)
    else:
        inputs = read_file(args.prompt_path)
        test_inputs = model_wise_formatter(args.model_type, inputs, args.abstract)

    outputs = inference(test_inputs, model, sampling_params)
    # pattern = r"Here's a modified version of the question that seeks more diverse (?:range)? of responses:\n(.*)"
    # cleaned_outputs = [re.sub(pattern, '', text) for text in outputs]

    cleaned_outputs = [output.split('\n')[1:] for output in outputs]
    if not os.path.exists(args.output_path):
        os.makedirs(os.path.join(args.output_path))
    
    dump_predictions(inputs, cleaned_outputs, os.path.join(args.output_path, args.model_type), args.model_path, args.dataset, args.abstract)



